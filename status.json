{
  "name": "@mtane0412\u306e\u30c4\u30a4\u30fc\u30c8 2024/09/03\u307e\u3067",
  "question": "@mtane0412\u306e\u5168\u30dd\u30b9\u30c8\u3092TTTC\u306b\u98df\u308f\u305b\u308b\u30c6\u30b9\u30c8",
  "input": "tweets20240903",
  "model": "gpt-4o",
  "extraction": {
    "workers": 3,
    "limit": 1000,
    "source_code": "import os\nimport json\nfrom tqdm import tqdm\nimport pandas as pd\nfrom langchain.chat_models import ChatOpenAI\nfrom utils import messages, update_progress\nimport concurrent.futures\n\n\ndef extraction(config):\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/args.csv\"\n    comments = pd.read_csv(f\"inputs/{config['input']}.csv\")\n\n    model = config['extraction']['model']\n    prompt = config['extraction']['prompt']\n    workers = config['extraction']['workers']\n    limit = config['extraction']['limit']\n\n    comment_ids = (comments['comment-id'].values)[:limit]\n    comments.set_index('comment-id', inplace=True)\n    results = pd.DataFrame()\n    update_progress(config, total=len(comment_ids))\n    for i in tqdm(range(0, len(comment_ids), workers)):\n        batch = comment_ids[i: i + workers]\n        batch_inputs = [comments.loc[id]['comment-body'] for id in batch]\n        batch_results = extract_batch(batch_inputs, prompt, model, workers)\n        for comment_id, extracted_args in zip(batch, batch_results):\n            for j, arg in enumerate(extracted_args):\n                new_row = {\"arg-id\": f\"A{comment_id}_{j}\",\n                           \"comment-id\": int(comment_id), \"argument\": arg}\n                results = pd.concat(\n                    [results, pd.DataFrame([new_row])], ignore_index=True)\n        update_progress(config, incr=len(batch))\n    results.to_csv(path, index=False)\n\n\ndef extract_batch(batch, prompt, model, workers):\n    with concurrent.futures.ThreadPoolExecutor(max_workers=workers) as executor:\n        futures = [executor.submit(\n            extract_arguments, input, prompt, model) for input in list(batch)]\n        concurrent.futures.wait(futures)\n        return [future.result() for future in futures]\n\n\ndef extract_arguments(input, prompt, model, retries=3):\n    llm = ChatOpenAI(model_name=model, temperature=0.0)\n    response = llm(messages=messages(prompt, input)).content.strip()\n    try:\n        obj = json.loads(response)\n        # LLM sometimes returns valid JSON string\n        if isinstance(obj, str):\n            obj = [obj]\n        items = [a.strip() for a in obj]\n        items = filter(None, items)  # omit empty strings\n        return items\n    except json.decoder.JSONDecodeError as e:\n        print(\"JSON error:\", e)\n        print(\"Input was:\", input)\n        print(\"Response was:\", response)\n        if retries > 0:\n            print(\"Retrying...\")\n            return extract_arguments(input, prompt, model, retries - 1)\n        else:\n            print(\"Silently giving up on trying to generate valid list.\")\n            return []\n",
    "prompt": "/system\n\n\n\u3042\u306a\u305f\u306f\u30d7\u30ed\u306e\u30ea\u30b5\u30fc\u30c1\u30fb\u30a2\u30b7\u30b9\u30bf\u30f3\u30c8\u3067\u3001\u79c1\u306e\u4ed5\u4e8b\u3092\u624b\u4f1d\u3046\u3053\u3068\u3067\u3059\u3002\n\u79c1\u306e\u4ed5\u4e8b\u306f\u3001\u8ad6\u70b9\u3092\u6574\u7406\u3057\u305f\u304d\u308c\u3044\u306a\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092\u4f5c\u6210\u3059\u308b\u3053\u3068\u3067\u3059\u3002\n\n\u80cc\u666f\u306f\u3001\u79c1\u306e\u4e0a\u53f8\u304c\u65e5\u672c\u306e\u90fd\u77e5\u4e8b\u9078\u6319\u306b\u7acb\u5019\u88dc\u3057\u305f\u3068\u3044\u3046\u3053\u3068\u3067\u3059\u3002\n\u3053\u308c\u304b\u3089\u3001\u51fa\u99ac\u306b\u95a2\u3057\u3066\u306e\u8a18\u4e8b\u306b\u4e00\u822c\u304b\u3089\u5bc4\u305b\u3089\u308c\u305f\u30b3\u30e1\u30f3\u30c8\u306e\u4f8b\u3092\u6319\u3052\u307e\u3059\u3002\n\u3088\u308a\u7c21\u6f54\u3067\u8aad\u307f\u3084\u3059\u3044\u3082\u306e\u306b\u3059\u308b\u306e\u3092\u624b\u4f1d\u3063\u3066\u307b\u3057\u3044\u3002\n\u672c\u5f53\u306b\u5fc5\u8981\u306a\u5834\u5408\u306f\u30012\u3064\u306e\u5225\u3005\u306e\u8b70\u8ad6\u306b\u5206\u3051\u308b\u3053\u3068\u3082\u3067\u304d\u308b\u304c\u30011\u3064\u306e\u8b70\u8ad6\u3092\u8fd4\u3059\u306e\u304c\u6700\u5584\u3067\u3042\u308b\u3053\u3068\u304c\u591a\u3044\u3060\u308d\u3046\u3002\n\n\u7d50\u679c\u306f\u3001\u304d\u3061\u3093\u3068\u30d5\u30a9\u30fc\u30de\u30c3\u30c8\u3055\u308c\u305f\u6587\u5b57\u5217\u5f62\u5f0f\uff08strings\uff09\u306eJSON\u30ea\u30b9\u30c8\u3068\u3057\u3066\u8fd4\u3057\u3066\u304f\u3060\u3055\u3044\u3002\n\n\n/human\n\nAI\u6280\u8853\u306f\u3001\u305d\u306e\u30e9\u30a4\u30d5\u30b5\u30a4\u30af\u30eb\u306b\u304a\u3051\u308b\u74b0\u5883\u8ca0\u8377\u306e\u4f4e\u6e1b\u306b\u91cd\u70b9\u3092\u7f6e\u3044\u3066\u958b\u767a\u3055\u308c\u308b\u3079\u304d\u3067\u3042\u308b\u3002\n\n/ai \n\n[\n  \"\u79c1\u305f\u3061\u306f\u3001AI\u6280\u8853\u304c\u74b0\u5883\u306b\u4e0e\u3048\u308b\u5f71\u97ff\u306e\u8efd\u6e1b\u306b\u7126\u70b9\u3092\u5f53\u3066\u308b\u3079\u304d\u3067\u3042\u308b\"\n]\n\n/human \n\nAI\u306e\u80fd\u529b\u3001\u9650\u754c\u3001\u502b\u7406\u7684\u914d\u616e\u306b\u3064\u3044\u3066\u4e00\u822c\u306e\u4eba\u3005\u3092\u6559\u80b2\u3059\u308b\u305f\u3081\u306e\u5354\u8abf\u7684\u306a\u52aa\u529b\u304c\u5fc5\u8981\u3067\u3042\u308b\u3002\n\n/ai \n\n[\n  \"AI\u306e\u80fd\u529b\u306b\u3064\u3044\u3066\u4e00\u822c\u306e\u4eba\u3005\u3092\u6559\u80b2\u3059\u3079\u304d\u3067\u3042\u308b\u3002\",\n  \"AI\u306e\u9650\u754c\u3068\u502b\u7406\u7684\u914d\u616e\u306b\u3064\u3044\u3066\u3001\u4e00\u822c\u306e\u4eba\u3005\u3092\u6559\u80b2\u3059\u3079\u304d\u3067\u3042\u308b\u3002\"\n]\n\n/human \n\nAI\u306f\u3001\u30a8\u30cd\u30eb\u30ae\u30fc\u52b9\u7387\u3068\u5c45\u4f4f\u8005\u306e\u30a6\u30a7\u30eb\u30d3\u30fc\u30a4\u30f3\u30b0\u306e\u305f\u3081\u306b\u30b9\u30de\u30fc\u30c8\u30db\u30fc\u30e0\u3084\u30d3\u30eb\u3092\u6700\u9069\u5316\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u308b\u3002\n\n/ ai \n\n[\n  \"AI\u306f\u3001\u30a8\u30cd\u30eb\u30ae\u30fc\u52b9\u7387\u3068\u5c45\u4f4f\u8005\u306e\u30a6\u30a7\u30eb\u30d3\u30fc\u30a4\u30f3\u30b0\u306e\u305f\u3081\u306b\u30b9\u30de\u30fc\u30c8\u30db\u30fc\u30e0\u3084\u30d3\u30eb\u3092\u6700\u9069\u5316\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u308b\u3002\"\n]\n\n/human \n\nAI\u306f\u30a8\u30cd\u30eb\u30ae\u30fc\u30fb\u30b0\u30ea\u30c3\u30c9\u3092\u6700\u9069\u5316\u3057\u3001\u7121\u99c4\u3068\u4e8c\u9178\u5316\u70ad\u7d20\u6392\u51fa\u3092\u524a\u6e1b\u3059\u308b\u306e\u306b\u5f79\u7acb\u3064\u3002\n\n/ai \n\n[\n  \"AI\u306f\u30a8\u30cd\u30eb\u30ae\u30fc\u7db2\u3092\u6700\u9069\u5316\u3057\u3001\u7121\u99c4\u3068\u4e8c\u9178\u5316\u70ad\u7d20\u6392\u51fa\u3092\u524a\u6e1b\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u308b\u3002\"\n]\n",
    "model": "gpt-4o"
  },
  "clustering": {
    "clusters": 8,
    "source_code": "\"\"\"Cluster the arguments using UMAP + HDBSCAN and GPT-4.\"\"\"\n\nimport pandas as pd\nimport numpy as np\nfrom importlib import import_module\n\n\ndef clustering(config):\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/clusters.csv\"\n    arguments_df = pd.read_csv(f\"outputs/{dataset}/args.csv\")\n    arguments_array = arguments_df[\"argument\"].values\n\n    embeddings_df = pd.read_pickle(f\"outputs/{dataset}/embeddings.pkl\")\n    embeddings_array = np.asarray(embeddings_df[\"embedding\"].values.tolist())\n    clusters = config['clustering']['clusters']\n\n    result = cluster_embeddings(\n        docs=arguments_array,\n        embeddings=embeddings_array,\n        metadatas={\n            \"arg-id\": arguments_df[\"arg-id\"].values,\n            \"comment-id\": arguments_df[\"comment-id\"].values,\n        },\n        n_topics=clusters,\n    )\n    result.to_csv(path, index=False)\n\n\ndef cluster_embeddings(\n    docs,\n    embeddings,\n    metadatas,\n    min_cluster_size=2,\n    n_components=2,\n    n_topics=6,\n):\n    # (!) we import the following modules dynamically for a reason\n    # (they are slow to load and not required for all pipelines)\n    SpectralClustering = import_module('sklearn.cluster').SpectralClustering\n    stopwords = import_module('nltk.corpus').stopwords\n    HDBSCAN = import_module('hdbscan').HDBSCAN\n    UMAP = import_module('umap').UMAP\n    CountVectorizer = import_module(\n        'sklearn.feature_extraction.text').CountVectorizer\n    BERTopic = import_module('bertopic').BERTopic\n\n    umap_model = UMAP(\n        random_state=42,\n        n_components=n_components,\n    )\n    hdbscan_model = HDBSCAN(min_cluster_size=min_cluster_size)\n\n    stop = stopwords.words(\"english\")\n    vectorizer_model = CountVectorizer(stop_words=stop)\n    topic_model = BERTopic(\n        umap_model=umap_model,\n        hdbscan_model=hdbscan_model,\n        vectorizer_model=vectorizer_model,\n        verbose=True,\n    )\n\n    # Fit the topic model.\n    _, __ = topic_model.fit_transform(docs, embeddings=embeddings)\n\n    n_samples = len(embeddings)\n    n_neighbors = min(n_samples - 1, 10)\n    spectral_model = SpectralClustering(\n        n_clusters=n_topics,\n        affinity=\"nearest_neighbors\",\n        n_neighbors=n_neighbors,  # Use the modified n_neighbors\n        random_state=42\n    )\n    umap_embeds = umap_model.fit_transform(embeddings)\n    cluster_labels = spectral_model.fit_predict(umap_embeds)\n\n    result = topic_model.get_document_info(\n        docs=docs,\n        metadata={\n            **metadatas,\n            \"x\": umap_embeds[:, 0],\n            \"y\": umap_embeds[:, 1],\n        },\n    )\n\n    result.columns = [c.lower() for c in result.columns]\n    result = result[['arg-id', 'x', 'y', 'probability']]\n    result['cluster-id'] = cluster_labels\n\n    return result\n"
  },
  "translation": {
    "model": "gpt-4o",
    "languages": [],
    "flags": [],
    "source_code": "import json\nfrom tqdm import tqdm\nimport pandas as pd\nfrom langchain.chat_models import ChatOpenAI\nfrom utils import messages, t\nfrom langchain.schema import AIMessage\nimport pandas as pd\nimport json\nfrom tqdm import tqdm\n\ndef translation(config):\n\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/translations.json\"\n    results = {}\n\n    languages = list(config.get('translation', {}).get('languages', []))\n    if len(languages) == 0:\n        print(\"No languages specified. Skipping translation step.\")\n        # creating an empty file any, to reduce special casing later\n        with open(path, 'w') as file:\n            json.dump(results, file, indent=2)\n        return\n\n    arguments = pd.read_csv(f\"outputs/{dataset}/args.csv\")\n    labels = pd.read_csv(f\"outputs/{dataset}/labels.csv\")\n    takeaways = pd.read_csv(f\"outputs/{dataset}/takeaways.csv\")\n    with open(f\"outputs/{dataset}/overview.txt\") as f:\n        overview = f.read()\n\n    UI_copy = [t(\"Argument\"), t(\"Original comment\"), t(\"Representative arguments\"),\n               t(\"Open full-screen map\"), t(\"Back to report\"), t(\"Hide labels\"), t(\"Show labels\"),\n               t(\"Show filters\"), t(\"Hide filters\"), t(\"Min. votes\"), t(\"Consensus\"),\n               t(\"Showing\"), t(\"arguments\"), t(\"Reset zoom\"), t(\"Click anywhere on the map to close this\"),\n               t(\"Click on the dot for details\"), t(\"agree\"), t(\"disagree\"), t(\"Language\"),\n               t(\"English\"), t(\"arguments\"), t(\"of total\"), t(\"Overview\"), t(\"Cluster analysis\"),\n               t(\"Representative comments\"), t(\"Introduction\"), t(\"Clusters\"), t(\"Appendix\"),\n               t(\"This report was generated using an AI pipeline that consists of the following steps\"),\n               t(\"Step\"), t(\"extraction\"), t(\"show code\"), t(\"hide code\"), t(\"show prompt\"),\n               t(\"hide prompt\"), t(\"embedding\"), t(\"clustering\"), t(\"labelling\"), t(\"takeaways\"),\n               t(\"overview\")]\n\n    arg_list = arguments['argument'].to_list() + \\\n        labels['label'].to_list() + \\\n        UI_copy + \\\n        languages\n\n    if 'name' in config:\n        arg_list.append(config['name'])\n    if 'question' in config:\n        arg_list.append(config['question'])\n\n    prompt_file = config.get('translation_prompt', 'default')\n    with open(f\"prompts/translation/{prompt_file}.txt\") as f:\n        prompt = f.read()\n    model = config['model']\n\n    config['translation_prompt'] = prompt\n\n    translations = [translate_lang(\n        arg_list, 10, prompt, lang, model) for lang in languages]\n\n    # handling long takeaways differently, WITHOUT batching too much\n    long_arg_list = takeaways['takeaways'].to_list()\n    long_arg_list.append(overview)\n    if 'intro' in config:\n        long_arg_list.append(config['intro'])\n\n    long_translations = [translate_lang(\n        long_arg_list, 1, prompt, lang, model) for lang in languages]\n\n    for i, id in enumerate(arg_list):\n        print('i, id', i, id)\n        results[str(id)] = list([t[i] for t in translations])\n    for i, id in enumerate(long_arg_list):\n        results[str(id)] = list([t[i] for t in long_translations])\n\n    with open(path, 'w') as file:\n        json.dump(results, file, indent=2)\n\n\ndef translate_lang(arg_list, batch_size, prompt, lang, model):\n    translations = []\n    lang_prompt = prompt.replace(\"{language}\", lang)\n    print(f\"Translating to {lang}...\")\n    for i in tqdm(range(0, len(arg_list), batch_size)):\n        batch = arg_list[i: i + batch_size]\n        translations.extend(translate_batch(batch, lang_prompt, model))\n    return translations\n\n\ndef translate_batch(batch, lang_prompt, model, retries=3):\n    llm = ChatOpenAI(model_name=model, temperature=0.0)\n    input = json.dumps(list(batch))\n    response = llm(messages=messages(lang_prompt, input)).content.strip()\n    if \"```\" in response:\n        response = response.split(\"```\")[1]\n    if response.startswith(\"json\"):\n        response = response[4:]\n    try:\n        parsed = [a.strip() for a in json.loads(response)]\n        if len(parsed) != len(batch):\n            print(\"Warning: batch size mismatch!\")\n            print(\"Batch len:\", len(batch))\n            print(\"Response len:\", len(parsed))\n            for i, item in enumerate(batch):\n                print(f\"Batch item {i}:\", item)\n                if (i < len(parsed)):\n                    print(\"Response:\", parsed[i])\n            if (len(batch) > 1):\n                print(\"Retrying with smaller batches...\")\n                mid = len(batch) // 2\n                return translate_batch(batch[:mid], lang_prompt, model, retries - 1) + \\\n                    translate_batch(\n                        batch[mid:], lang_prompt, model, retries - 1)\n            else:\n                print(\"Retrying batch...\")\n                return translate_batch(batch, lang_prompt, model, retries - 1)\n        else:\n            return parsed\n    except json.decoder.JSONDecodeError as e:\n        print(\"JSON error:\", e)\n        print(\"Response was:\", response)\n        if retries > 0:\n            print(\"Retrying batch...\")\n            return translate_batch(batch, lang_prompt, model, retries - 1)\n        else:\n            raise e",
    "prompt": "/system \n\n\u3042\u306a\u305f\u306f\u30d7\u30ed\u306e\u7ffb\u8a33\u8005\u3067\u3059\u3002\n\u65e5\u672c\u8a9e\u3067\u66f8\u304b\u308c\u305f\u5358\u8a9e\u3068\u6587\u7ae0\u306e\u30ea\u30b9\u30c8\u3092\u53d7\u3051\u53d6\u308a\u307e\u3059\u3002\n\u540c\u3058\u30ea\u30b9\u30c8\u3092\u540c\u3058\u9806\u756a\u3067\u3001{language}\u306b\u7ffb\u8a33\u3057\u3066\u8fd4\u3057\u3066\u304f\u3060\u3055\u3044\u3002\n\u5143\u306e\u30ea\u30b9\u30c8\u3068\u540c\u3058\u9577\u3055\u306e\u6587\u5b57\u5217\u306e\u6709\u52b9\u306aJSON\u30ea\u30b9\u30c8\u3092\u8fd4\u3059\u3088\u3046\u306b\u3057\u3066\u304f\u3060\u3055\u3044\u3002"
  },
  "intro": "\u3053\u306eAI\u304c\u4f5c\u6210\u3057\u305f\u30ec\u30dd\u30fc\u30c8\u306f\u30012024\u5e749\u67083\u65e5\u307e\u3067\u306eX\u306e\u6295\u7a3f\u30c7\u30fc\u30bf\u306b\u4f9d\u62e0\u3057\u3066\u3044\u308b\u3002",
  "output_dir": "tweets20240903",
  "embedding": {
    "source_code": "\nfrom langchain.embeddings import OpenAIEmbeddings\nimport pandas as pd\nfrom tqdm import tqdm\n\n\ndef embedding(config):\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/embeddings.pkl\"\n    arguments = pd.read_csv(f\"outputs/{dataset}/args.csv\")\n    embeddings = []\n    for i in tqdm(range(0, len(arguments), 1000)):\n        args = arguments[\"argument\"].tolist()[i: i + 1000]\n        embeds = OpenAIEmbeddings().embed_documents(args)\n        embeddings.extend(embeds)\n    df = pd.DataFrame(\n        [\n            {\"arg-id\": arguments.iloc[i][\"arg-id\"], \"embedding\": e}\n            for i, e in enumerate(embeddings)\n        ]\n    )\n    df.to_pickle(path)\n"
  },
  "labelling": {
    "sample_size": 30,
    "source_code": "\"\"\"Create labels for the clusters.\"\"\"\n\nfrom tqdm import tqdm\nfrom typing import List\nimport numpy as np\nimport pandas as pd\nfrom langchain.chat_models import ChatOpenAI\nfrom utils import messages, update_progress\n\n\ndef labelling(config):\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/labels.csv\"\n\n    arguments = pd.read_csv(f\"outputs/{dataset}/args.csv\")\n    clusters = pd.read_csv(f\"outputs/{dataset}/clusters.csv\")\n\n    results = pd.DataFrame()\n\n    sample_size = config['labelling']['sample_size']\n    prompt = config['labelling']['prompt']\n    model = config['labelling']['model']\n\n    question = config['question']\n    cluster_ids = clusters['cluster-id'].unique()\n\n    update_progress(config, total=len(cluster_ids))\n\n    for _, cluster_id in tqdm(enumerate(cluster_ids), total=len(cluster_ids)):\n        args_ids = clusters[clusters['cluster-id']\n                            == cluster_id]['arg-id'].values\n        args_ids = np.random.choice(args_ids, size=min(\n            len(args_ids), sample_size), replace=False)\n        args_sample = arguments[arguments['arg-id']\n                                .isin(args_ids)]['argument'].values\n\n        args_ids_outside = clusters[clusters['cluster-id']\n                                    != cluster_id]['arg-id'].values\n        args_ids_outside = np.random.choice(args_ids_outside, size=min(\n            len(args_ids_outside), sample_size), replace=False)\n        args_sample_outside = arguments[arguments['arg-id']\n                                        .isin(args_ids_outside)]['argument'].values\n\n        label = generate_label(question, args_sample,\n                               args_sample_outside, prompt, model)\n        results = pd.concat([results, pd.DataFrame(\n            [{'cluster-id': cluster_id, 'label': label}])], ignore_index=True)\n        update_progress(config, incr=1)\n\n    results.to_csv(path, index=False)\n\n\ndef generate_label(question, args_sample, args_sample_outside, prompt, model):\n    llm = ChatOpenAI(model_name=model, temperature=0.0)\n    outside = '\\n * ' + '\\n * '.join(args_sample_outside)\n    inside = '\\n * ' + '\\n * '.join(args_sample)\n    input = f\"Question of the consultation:{question}\\n\\n\" + \\\n        f\"Examples of arguments OUTSIDE the cluster:\\n {outside}\" + \\\n        f\"Examples of arguments INSIDE the cluster:\\n {inside}\"\n    response = llm(messages=messages(prompt, input)).content.strip()\n    return response\n",
    "prompt": "/system \n\n\u3042\u306a\u305f\u306f\u3001\u3088\u308a\u5e83\u7bc4\u306a\u30b3\u30f3\u30b5\u30eb\u30c6\u30fc\u30b7\u30e7\u30f3\u5185\u306e\u4e00\u9023\u306e\u8b70\u8ad6\u306b\u5bfe\u3059\u308b\u30ab\u30c6\u30b4\u30ea\u30e9\u30d9\u30eb\u3092\u751f\u6210\u3059\u308b\u30ab\u30c6\u30b4\u30ea\u30e9\u30d9\u30ea\u30f3\u30b0\u30a2\u30b7\u30b9\u30bf\u30f3\u30c8\u3067\u3059\u3002\u3042\u306a\u305f\u306b\u306f\u3001\u76f8\u8ac7\u306e\u4e3b\u306a\u8cea\u554f\u3001\u30af\u30e9\u30b9\u30bf\u5185\u306e\u8b70\u8ad6\u306e\u30ea\u30b9\u30c8\u3001\u304a\u3088\u3073\u3053\u306e\u30af\u30e9\u30b9\u30bf\u5916\u306e\u8b70\u8ad6\u306e\u30ea\u30b9\u30c8\u304c\u4e0e\u3048\u3089\u308c\u307e\u3059\u3002\u3042\u306a\u305f\u306f\u30af\u30e9\u30b9\u30bf\u30fc\u3092\u8981\u7d04\u3059\u308b1\u3064\u306e\u30ab\u30c6\u30b4\u30ea\u30fc\u30e9\u30d9\u30eb\u3067\u56de\u7b54\u3057\u307e\u3059\u3002\n\n\u3042\u306a\u305f\u306f\u3001\u3088\u308a\u5e83\u7bc4\u306a\u30b3\u30f3\u30b5\u30eb\u30c6\u30fc\u30b7\u30e7\u30f3\u5185\u306e\u4e00\u9023\u306e\u8b70\u8ad6\u306b\u5bfe\u3059\u308b\u30ab\u30c6\u30b4\u30ea\u30e9\u30d9\u30eb\u3092\u751f\u6210\u3059\u308b\u30ab\u30c6\u30b4\u30ea\u30e9\u30d9\u30ea\u30f3\u30b0\u30a2\u30b7\u30b9\u30bf\u30f3\u30c8\u3067\u3059\u3002\u3042\u306a\u305f\u306b\u306f\u3001\u76f8\u8ac7\u306e\u4e3b\u306a\u8cea\u554f\u3001\u30af\u30e9\u30b9\u30bf\u5185\u306e\u8b70\u8ad6\u306e\u30ea\u30b9\u30c8\u3001\u304a\u3088\u3073\u3053\u306e\u30af\u30e9\u30b9\u30bf\u5916\u306e\u8b70\u8ad6\u306e\u30ea\u30b9\u30c8\u304c\u4e0e\u3048\u3089\u308c\u307e\u3059\u3002\u3042\u306a\u305f\u306f\u30af\u30e9\u30b9\u30bf\u30fc\u3092\u8981\u7d04\u3059\u308b1\u3064\u306e\u30ab\u30c6\u30b4\u30ea\u30fc\u30e9\u30d9\u30eb\u3067\u56de\u7b54\u3057\u307e\u3059\u3002\n\n\u30e9\u30d9\u30eb\u306f\u975e\u5e38\u306b\u7c21\u6f54\u3067\u306a\u3051\u308c\u3070\u306a\u3089\u305a\u3001\u30af\u30e9\u30b9\u30bf\u30fc\u3068\u305d\u306e\u5916\u5074\u306b\u3042\u308b\u8ad6\u70b9\u3092\u533a\u5225\u3059\u308b\u306e\u306b\u5341\u5206\u306a\u6b63\u78ba\u3055\u3067\u306a\u3051\u308c\u3070\u306a\u3089\u306a\u3044\u3002\n\n/human\n\n\u30b3\u30f3\u30b5\u30eb\u30c6\u30fc\u30b7\u30e7\u30f3\u306e\u8cea\u554f \u300c\u82f1\u56fd\u306eEU\u96e2\u8131\u6c7a\u5b9a\u306e\u5f71\u97ff\u306f\u4f55\u3060\u3068\u601d\u3044\u307e\u3059\u304b\uff1f\n\n\u95a2\u5fc3\u306e\u3042\u308b\u30af\u30e9\u30b9\u30bf\u30fc\u4ee5\u5916\u306e\u8ad6\u70b9\u306e\u4f8b\n\n * \u30a8\u30e9\u30b9\u30e0\u30b9\u30fb\u30d7\u30ed\u30b0\u30e9\u30e0\u304b\u3089\u306e\u9664\u5916\u306b\u3088\u308a\u3001\u6559\u80b2\u30fb\u6587\u5316\u4ea4\u6d41\u306e\u6a5f\u4f1a\u304c\u5236\u9650\u3055\u308c\u305f\u3002\n * \u82f1\u56fd\u306f\u3001\u56fd\u5883\u691c\u554f\u306e\u5f37\u5316\u306b\u3088\u308b\u65c5\u884c\u6642\u9593\u306e\u5ef6\u9577\u306b\u5bfe\u51e6\u3057\u3001\u901a\u52e4\u5ba2\u3084\u65c5\u884c\u5ba2\u306b\u5f71\u97ff\u3092\u4e0e\u3048\u305f\u3002\n * \u74b0\u5883\u57fa\u6e96\u306b\u304a\u3051\u308b\u5354\u529b\u304c\u6e1b\u5c11\u3057\u3001\u6c17\u5019\u5909\u52d5\u3068\u95d8\u3046\u52aa\u529b\u304c\u59a8\u3052\u3089\u308c\u305f\u3002\n * \u76f8\u4e92\u533b\u7642\u5354\u5b9a\u306e\u4e2d\u65ad\u306b\u3088\u308a\u3001\u60a3\u8005\u30b1\u30a2\u306b\u8ab2\u984c\u3092\u611f\u3058\u305f\u3002\n * Brexit\u95a2\u9023\u306e\u5909\u66f4\u306b\u3088\u308a\u3001\u5bb6\u65cf\u306e\u5c45\u4f4f\u6a29\u3084\u5e02\u6c11\u6a29\u306e\u7533\u8acb\u304c\u8907\u96d1\u306b\u306a\u3063\u305f\u3002\n * \u82f1\u56fd\u306f\u3001\u5171\u540c\u7814\u7a76\u6a5f\u4f1a\u306e\u6e1b\u5c11\u306b\u3088\u308a\u3001\u7814\u7a76\u306e\u8ab2\u984c\u306b\u53d6\u308a\u7d44\u3080\u4e16\u754c\u7684\u306a\u53d6\u308a\u7d44\u307f\u306b\u652f\u969c\u3092\u304d\u305f\u3059\u3053\u3068\u3092\u76ee\u306e\u5f53\u305f\u308a\u306b\u3057\u305f\u3002\n * EU\u306e\u6587\u5316\u52a9\u6210\u30d7\u30ed\u30b0\u30e9\u30e0\u304b\u3089\u306e\u9664\u5916\u306b\u3088\u308a\u3001\u5275\u9020\u7684\u306a\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u306e\u5236\u9650\u306b\u76f4\u9762\u3057\u305f\u3002\n * \u82f1\u56fd\u306f\u3001EU\u306e\u8cc7\u91d1\u63d0\u4f9b\u306e\u55aa\u5931\u306b\u3088\u308a\u3001\u6148\u5584\u6d3b\u52d5\u3084\u30b3\u30df\u30e5\u30cb\u30c6\u30a3\u652f\u63f4\u306e\u5f8c\u9000\u3092\u76ee\u306e\u5f53\u305f\u308a\u306b\u3057\u305f\u3002\n * \u6d88\u8cbb\u8005\u4fdd\u8b77\u306e\u5f31\u4f53\u5316\u306b\u3088\u308a\u3001\u56fd\u5883\u3092\u8d8a\u3048\u305f\u7d1b\u4e89\u89e3\u6c7a\u306b\u8ab2\u984c\u304c\u751f\u3058\u305f\u3002\n * \u82f1\u56fd\u306f\u30d7\u30ed\u306e\u97f3\u697d\u5bb6\u3068\u3057\u3066EU\u8af8\u56fd\u3092\u30c4\u30a2\u30fc\u3059\u308b\u969b\u306e\u5236\u9650\u306b\u76f4\u9762\u3057\u3001\u30ad\u30e3\u30ea\u30a2\u306b\u5f71\u97ff\u3092\u4e0e\u3048\u305f\u3002\n\n\u30af\u30e9\u30b9\u30bf\u30fc\u5185\u90e8\u3067\u306e\u8b70\u8ad6\u306e\u4f8b\n\n * Brexit\u306b\u3088\u308a\u30b5\u30d7\u30e9\u30a4\u30c1\u30a7\u30fc\u30f3\u304c\u6df7\u4e71\u3057\u3001\u4f01\u696d\u306b\u3068\u3063\u3066\u30b3\u30b9\u30c8\u5897\u3068\u7d0d\u671f\u9045\u5ef6\u306b\u3064\u306a\u304c\u3063\u305f\u3002\n * \u30d6\u30ec\u30b0\u30b8\u30c3\u30c8\u306e\u305f\u3081\u3001\u5e02\u5834\u306e\u5909\u52d5\u3084\u6295\u8cc7\u30fb\u9000\u8077\u91d1\u306e\u4e0d\u78ba\u5b9f\u6027\u306b\u76f4\u9762\u3057\u305f\u3002\n * \u65b0\u305f\u306a\u95a2\u7a0e\u3084\u901a\u95a2\u624b\u7d9a\u304d\u306b\u3088\u308a\u3001\u82f1\u56fd\u306f\u8f38\u51fa\u696d\u8005\u3068\u3057\u3066\u5229\u76ca\u7387\u306e\u4f4e\u4e0b\u306b\u5bfe\u51e6\u3057\u305f\u3002\n * \u30d6\u30ec\u30b0\u30b8\u30c3\u30c8\u5f8c\u3001\u4f01\u696d\u304cEU\u5e02\u5834\u5185\u306b\u3068\u3069\u307e\u308b\u305f\u3081\u306b\u4e8b\u696d\u3092\u79fb\u8ee2\u3057\u305f\u305f\u3081\u3001\u96c7\u7528\u3092\u5931\u3063\u305f\u3002\n * \u82f1\u56fd\u306f\u8f38\u5165\u54c1\u4fa1\u683c\u306e\u9ad8\u9a30\u306b\u3088\u308b\u751f\u6d3b\u8cbb\u306e\u5897\u52a0\u306b\u82e6\u3057\u3093\u3060\u3002\n * \u82f1\u56fd\u306e\u30cf\u30a4\u30c6\u30af\u7523\u696d\u3078\u306e\u6295\u8cc7\u304c\u6e1b\u5c11\u3057\u3001\u6280\u8853\u9769\u65b0\u3068\u96c7\u7528\u6a5f\u4f1a\u306b\u5f71\u97ff\u3092\u4e0e\u3048\u305f\u3002\n * \u65b0\u305f\u306a\u30d3\u30b6\u898f\u5236\u306b\u3088\u308b\u89b3\u5149\u5ba2\u306e\u6e1b\u5c11\u3092\u76ee\u306e\u5f53\u305f\u308a\u306b\u3057\u3001\u63a5\u5ba2\u696d\u306b\u5f71\u97ff\u3002\n * \u30dd\u30f3\u30c9\u4fa1\u5024\u306e\u4e0b\u843d\u306b\u3088\u308a\u8cfc\u8cb7\u529b\u304c\u4f4e\u4e0b\u3057\u3001\u65c5\u8cbb\u304c\u5897\u52a0\u3057\u305f\u3002\n\n\n/ai \n\n\u8ca1\u52d9\u4e0a\u306e\u30de\u30a4\u30ca\u30b9\u5f71\u97ff",
    "model": "gpt-4o"
  },
  "takeaways": {
    "sample_size": 30,
    "source_code": "\"\"\"Create summaries for the clusters.\"\"\"\n\nfrom tqdm import tqdm\nimport os\nfrom typing import List\nimport numpy as np\nimport pandas as pd\nfrom langchain.chat_models import ChatOpenAI\nfrom utils import messages, update_progress\n\n\ndef takeaways(config):\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/takeaways.csv\"\n\n    arguments = pd.read_csv(f\"outputs/{dataset}/args.csv\")\n    clusters = pd.read_csv(f\"outputs/{dataset}/clusters.csv\")\n\n    results = pd.DataFrame()\n\n    sample_size = config['takeaways']['sample_size']\n    prompt = config['takeaways']['prompt']\n    model = config['takeaways']['model']\n\n    model = config.get('model_takeaways', config.get('model', 'gpt3.5-turbo'))\n    cluster_ids = clusters['cluster-id'].unique()\n\n    update_progress(config, total=len(cluster_ids))\n\n    for _, cluster_id in tqdm(enumerate(cluster_ids), total=len(cluster_ids)):\n        args_ids = clusters[clusters['cluster-id']\n                            == cluster_id]['arg-id'].values\n        args_ids = np.random.choice(args_ids, size=min(\n            len(args_ids), sample_size), replace=False)\n        args_sample = arguments[arguments['arg-id']\n                                .isin(args_ids)]['argument'].values\n        label = generate_takeaways(args_sample, prompt, model)\n        results = pd.concat([results, pd.DataFrame(\n            [{'cluster-id': cluster_id, 'takeaways': label}])], ignore_index=True)\n        update_progress(config, incr=1)\n\n    results.to_csv(path, index=False)\n\n\ndef generate_takeaways(args_sample, prompt, model):\n    llm = ChatOpenAI(model_name=model, temperature=0.0)\n    input = \"\\n\".join(args_sample)\n    response = llm(messages=messages(prompt, input)).content.strip()\n    return response\n",
    "prompt": "/system \n\n\u3042\u306a\u305f\u306f\u30b7\u30f3\u30af\u30bf\u30f3\u30af\u3067\u50cd\u304f\u5c02\u9580\u5bb6\u30ea\u30b5\u30fc\u30c1\u30a2\u30b7\u30b9\u30bf\u30f3\u30c8\u3067\u3059\u3002\u3042\u306a\u305f\u306f\u3001\u516c\u958b\u5354\u8b70\u306e\u969b\u306b\u53c2\u52a0\u8005\u306e\u4e00\u56e3\u304b\u3089\u51fa\u3055\u308c\u305f\u8b70\u8ad6\u306e\u30ea\u30b9\u30c8\u3092\u6e21\u3055\u308c\u307e\u3059\u3002\u3042\u306a\u305f\u306f\u3001\u4e3b\u306a\u8ad6\u70b9\u30921\uff5e2\u6bb5\u843d\u306b\u307e\u3068\u3081\u3066\u56de\u7b54\u3057\u307e\u3059\u3002\u3042\u306a\u305f\u306f\u3068\u3066\u3082\u7c21\u6f54\u3067\u3001\u8aad\u307f\u3084\u3059\u3044\u77ed\u3044\u6587\u7ae0\u3092\u66f8\u304f\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002\n \n/human\n\n[\n  \"\u9283\u306b\u3088\u308b\u66b4\u529b\u306f\u3001\u79c1\u305f\u3061\u306e\u793e\u4f1a\u306b\u304a\u3051\u308b\u6df1\u523b\u306a\u516c\u8846\u885b\u751f\u306e\u5371\u6a5f\u3092\u69cb\u6210\u3057\u3066\u3044\u308b\u3068\u56fa\u304f\u4fe1\u3058\u3066\u3044\u307e\u3059\u3002\",\n  \"\u5305\u62ec\u7684\u306a\u9283\u898f\u5236\u7b56\u3092\u901a\u3058\u3066\u3001\u3053\u306e\u554f\u984c\u306b\u65e9\u6025\u306b\u53d6\u308a\u7d44\u3080\u5fc5\u8981\u304c\u3042\u308b\u3002\",\n  \"\u3059\u3079\u3066\u306e\u9283\u8cfc\u5165\u8005\u306b\u5bfe\u3059\u308b\u8eab\u5143\u8abf\u67fb\u306e\u5b9f\u65bd\u3092\u652f\u6301\u3057\u307e\u3059\u3002\",\n  \"\u30a2\u30b5\u30eb\u30c8\u30fb\u30a6\u30a7\u30dd\u30f3\u3068\u5927\u5bb9\u91cf\u5f3e\u5009\u306e\u7981\u6b62\u306b\u8cdb\u6210\u3057\u307e\u3059\u3002\",\n  \"\u9055\u6cd5\u306a\u9283\u306e\u58f2\u8cb7\u3092\u9632\u3050\u305f\u3081\u3001\u3088\u308a\u53b3\u3057\u3044\u898f\u5236\u3092\u63d0\u5531\u3057\u307e\u3059\u3002\",\n  \"\u9283\u306e\u8cfc\u5165\u30d7\u30ed\u30bb\u30b9\u306b\u304a\u3044\u3066\u3001\u7cbe\u795e\u9451\u5b9a\u3092\u7fa9\u52d9\u4ed8\u3051\u308b\u3079\u304d\u3067\u3042\u308b\u3002\"\n]\n\n/ai \n\n\u53c2\u52a0\u8005\u306f\u3001\u5305\u62ec\u7684\u306a\u9283\u898f\u5236\u3092\u6c42\u3081\u3001\u666e\u904d\u7684\u306a\u8eab\u5143\u8abf\u67fb\u3001\u7a81\u6483\u5175\u5668\u306e\u7981\u6b62\u3001\u9055\u6cd5\u306a\u9283\u58f2\u8cb7\u306e\u6291\u5236\u3001\u7cbe\u795e\u885b\u751f\u8a55\u4fa1\u306e\u512a\u5148\u306a\u3069\u3092\u5f37\u8abf\u3057\u305f\u3002",
    "model": "gpt-4o"
  },
  "overview": {
    "source_code": "\"\"\"Create summaries for the clusters.\"\"\"\n\nfrom tqdm import tqdm\nimport os\nfrom typing import List\nimport numpy as np\nimport pandas as pd\nfrom langchain.chat_models import ChatOpenAI\nfrom utils import messages, update_progress\n\n\ndef overview(config):\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/overview.txt\"\n\n    takeaways = pd.read_csv(f\"outputs/{dataset}/takeaways.csv\")\n    labels = pd.read_csv(f\"outputs/{dataset}/labels.csv\")\n\n    prompt = config['overview']['prompt']\n    model = config['overview']['model']\n\n    ids = labels['cluster-id'].to_list()\n    takeaways.set_index('cluster-id', inplace=True)\n    labels.set_index('cluster-id', inplace=True)\n\n    input = ''\n    for i, id in enumerate(ids):\n        input += f\"# Cluster {i}/{len(ids)}: {labels.loc[id]['label']}\\n\\n\"\n        input += takeaways.loc[id]['takeaways'] + '\\n\\n'\n\n    llm = ChatOpenAI(model_name=model, temperature=0.0)\n    response = llm(messages=messages(prompt, input)).content.strip()\n\n    with open(path, 'w') as file:\n        file.write(response)\n",
    "prompt": "/system \n\n\u3042\u306a\u305f\u306f\u30b7\u30f3\u30af\u30bf\u30f3\u30af\u3067\u50cd\u304f\u5c02\u9580\u5bb6\u30ea\u30b5\u30fc\u30c1\u30a2\u30b7\u30b9\u30bf\u30f3\u30c8\u3067\u3059\u3002\n\u3042\u306a\u305f\u306e\u30c1\u30fc\u30e0\u306f\u3001\u3042\u308b\u30c8\u30d4\u30c3\u30af\u306b\u95a2\u3059\u308b\u516c\u958b\u30b3\u30f3\u30b5\u30eb\u30c6\u30fc\u30b7\u30e7\u30f3\u3092\u5b9f\u65bd\u3057\u3001\u3055\u307e\u3056\u307e\u306a\u9078\u629e\u80a2\u306e\u30af\u30e9\u30b9\u30bf\u30fc\u3092\u5206\u6790\u3057\u59cb\u3081\u307e\u3057\u305f\u3002\n\u3042\u306a\u305f\u306f\u4eca\u3001\u30af\u30e9\u30b9\u30bf\u30fc\u306e\u30ea\u30b9\u30c8\u3068\u5404\u30af\u30e9\u30b9\u30bf\u30fc\u306e\u7c21\u5358\u306a\u5206\u6790\u3092\u53d7\u3051\u53d6\u308a\u307e\u3059\u3002\n\u3042\u306a\u305f\u306e\u4ed5\u4e8b\u306f\u3001\u305d\u306e\u7d50\u679c\u3092\u7c21\u6f54\u306b\u307e\u3068\u3081\u308b\u3053\u3068\u3067\u3059\u3002\n\u3042\u306a\u305f\u306e\u8981\u7d04\u306f\u975e\u5e38\u306b\u7c21\u6f54\u3067\u306a\u3051\u308c\u3070\u306a\u3089\u305a\uff08\u305b\u3044\u305c\u30441\u6bb5\u843d\u3001\u305b\u3044\u305c\u30444\u6587\uff09\u3001\u5e73\u51e1\u306a\u8868\u73fe\u306f\u907f\u3051\u306a\u3051\u308c\u3070\u306a\u308a\u307e\u305b\u3093\u3002",
    "model": "gpt-4o"
  },
  "aggregation": {
    "source_code": "\"\"\"Generate a convenient JSON output file.\"\"\"\n\nfrom tqdm import tqdm\nfrom typing import List\nimport pandas as pd\nfrom langchain.chat_models import ChatOpenAI\nimport json\n\n\ndef aggregation(config):\n\n    path = f\"outputs/{config['output_dir']}/result.json\"\n\n    results = {\n        \"clusters\": [],\n        \"comments\": {},\n        \"translations\": {},\n        \"overview\": \"\",\n        \"config\": config,\n    }\n\n    arguments = pd.read_csv(f\"outputs/{config['output_dir']}/args.csv\")\n    arguments.set_index('arg-id', inplace=True)\n\n    comments = pd.read_csv(f\"inputs/{config['input']}.csv\")\n    useful_comment_ids = set(arguments['comment-id'].values)\n    for _, row in comments.iterrows():\n        id = row['comment-id']\n        if id in useful_comment_ids:\n            res = {'comment': row['comment-body']}\n            numeric_cols = ['agrees', 'disagrees']\n            string_cols = ['video', 'interview', 'timestamp']\n            for col in numeric_cols:\n                if col in row:\n                    res[col] = float(row[col])\n            for col in string_cols:\n                if col in row:\n                    res[col] = row[col]\n            results['comments'][str(id)] = res\n\n    languages = list(config.get('translation', {}).get('languages', []))\n    if len(languages) > 0:\n        with open(f\"outputs/{config['output_dir']}/translations.json\") as f:\n            translations = f.read()\n        results['translations'] = json.loads(translations)\n\n    clusters = pd.read_csv(f\"outputs/{config['output_dir']}/clusters.csv\")\n    labels = pd.read_csv(f\"outputs/{config['output_dir']}/labels.csv\")\n    takeaways = pd.read_csv(f\"outputs/{config['output_dir']}/takeaways.csv\")\n    takeaways.set_index('cluster-id', inplace=True)\n\n    with open(f\"outputs/{config['output_dir']}/overview.txt\") as f:\n        overview = f.read()\n    results['overview'] = overview\n\n    for _, row in labels.iterrows():\n        cid = row['cluster-id']\n        label = row['label']\n        arg_rows = clusters[clusters['cluster-id'] == cid]\n        arguments_in_cluster = []\n        for _, arg_row in arg_rows.iterrows():\n            arg_id = arg_row['arg-id']\n            argument = arguments.loc[arg_id]['argument']\n            comment_id = arguments.loc[arg_id]['comment-id']\n            x = float(arg_row['x'])\n            y = float(arg_row['y'])\n            p = float(arg_row['probability'])\n            obj = {\n                'arg_id': arg_id,\n                'argument': argument,\n                'comment_id': str(comment_id),\n                'x': x,\n                'y': y,\n                'p': p,\n            }\n            arguments_in_cluster.append(obj)\n        results['clusters'].append({\n            'cluster': label,\n            'cluster_id': str(cid),\n            'takeaways': takeaways.loc[cid]['takeaways'],\n            'arguments': arguments_in_cluster\n        })\n\n    with open(path, 'w') as file:\n        json.dump(results, file, indent=2)\n"
  },
  "visualization": {
    "replacements": [],
    "source_code": "\nimport subprocess\n\n\ndef visualization(config):\n    output_dir = config['output_dir']\n    with open(f\"outputs/{output_dir}/result.json\") as f:\n        result = f.read()\n\n    cwd = \"../next-app\"\n    command = f\"REPORT={output_dir} npm run build\"\n\n    try:\n        process = subprocess.Popen(command, shell=True, cwd=cwd, stdout=subprocess.PIPE,\n                                   stderr=subprocess.PIPE, universal_newlines=True)\n        while True:\n            output_line = process.stdout.readline()\n            if output_line == '' and process.poll() is not None:\n                break\n            if output_line:\n                print(output_line.strip())\n        process.wait()\n        errors = process.stderr.read()\n        if errors:\n            print(\"Errors:\")\n            print(errors)\n    except subprocess.CalledProcessError as e:\n        print(\"Error: \", e)\n"
  },
  "plan": [
    {
      "step": "extraction",
      "run": true,
      "reason": "not trace of previous run"
    },
    {
      "step": "embedding",
      "run": true,
      "reason": "not trace of previous run"
    },
    {
      "step": "clustering",
      "run": true,
      "reason": "not trace of previous run"
    },
    {
      "step": "labelling",
      "run": true,
      "reason": "not trace of previous run"
    },
    {
      "step": "takeaways",
      "run": true,
      "reason": "not trace of previous run"
    },
    {
      "step": "overview",
      "run": true,
      "reason": "not trace of previous run"
    },
    {
      "step": "translation",
      "run": true,
      "reason": "not trace of previous run"
    },
    {
      "step": "aggregation",
      "run": true,
      "reason": "not trace of previous run"
    },
    {
      "step": "visualization",
      "run": true,
      "reason": "not trace of previous run"
    }
  ],
  "status": "completed",
  "start_time": "2024-09-04T20:29:54.339785",
  "completed_jobs": [
    {
      "step": "extraction",
      "completed": "2024-09-04T20:50:08.478115",
      "duration": 1214.136204,
      "params": {
        "workers": 3,
        "limit": 1000,
        "source_code": "import os\nimport json\nfrom tqdm import tqdm\nimport pandas as pd\nfrom langchain.chat_models import ChatOpenAI\nfrom utils import messages, update_progress\nimport concurrent.futures\n\n\ndef extraction(config):\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/args.csv\"\n    comments = pd.read_csv(f\"inputs/{config['input']}.csv\")\n\n    model = config['extraction']['model']\n    prompt = config['extraction']['prompt']\n    workers = config['extraction']['workers']\n    limit = config['extraction']['limit']\n\n    comment_ids = (comments['comment-id'].values)[:limit]\n    comments.set_index('comment-id', inplace=True)\n    results = pd.DataFrame()\n    update_progress(config, total=len(comment_ids))\n    for i in tqdm(range(0, len(comment_ids), workers)):\n        batch = comment_ids[i: i + workers]\n        batch_inputs = [comments.loc[id]['comment-body'] for id in batch]\n        batch_results = extract_batch(batch_inputs, prompt, model, workers)\n        for comment_id, extracted_args in zip(batch, batch_results):\n            for j, arg in enumerate(extracted_args):\n                new_row = {\"arg-id\": f\"A{comment_id}_{j}\",\n                           \"comment-id\": int(comment_id), \"argument\": arg}\n                results = pd.concat(\n                    [results, pd.DataFrame([new_row])], ignore_index=True)\n        update_progress(config, incr=len(batch))\n    results.to_csv(path, index=False)\n\n\ndef extract_batch(batch, prompt, model, workers):\n    with concurrent.futures.ThreadPoolExecutor(max_workers=workers) as executor:\n        futures = [executor.submit(\n            extract_arguments, input, prompt, model) for input in list(batch)]\n        concurrent.futures.wait(futures)\n        return [future.result() for future in futures]\n\n\ndef extract_arguments(input, prompt, model, retries=3):\n    llm = ChatOpenAI(model_name=model, temperature=0.0)\n    response = llm(messages=messages(prompt, input)).content.strip()\n    try:\n        obj = json.loads(response)\n        # LLM sometimes returns valid JSON string\n        if isinstance(obj, str):\n            obj = [obj]\n        items = [a.strip() for a in obj]\n        items = filter(None, items)  # omit empty strings\n        return items\n    except json.decoder.JSONDecodeError as e:\n        print(\"JSON error:\", e)\n        print(\"Input was:\", input)\n        print(\"Response was:\", response)\n        if retries > 0:\n            print(\"Retrying...\")\n            return extract_arguments(input, prompt, model, retries - 1)\n        else:\n            print(\"Silently giving up on trying to generate valid list.\")\n            return []\n",
        "prompt": "/system\n\n\n\u3042\u306a\u305f\u306f\u30d7\u30ed\u306e\u30ea\u30b5\u30fc\u30c1\u30fb\u30a2\u30b7\u30b9\u30bf\u30f3\u30c8\u3067\u3001\u79c1\u306e\u4ed5\u4e8b\u3092\u624b\u4f1d\u3046\u3053\u3068\u3067\u3059\u3002\n\u79c1\u306e\u4ed5\u4e8b\u306f\u3001\u8ad6\u70b9\u3092\u6574\u7406\u3057\u305f\u304d\u308c\u3044\u306a\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092\u4f5c\u6210\u3059\u308b\u3053\u3068\u3067\u3059\u3002\n\n\u80cc\u666f\u306f\u3001\u79c1\u306e\u4e0a\u53f8\u304c\u65e5\u672c\u306e\u90fd\u77e5\u4e8b\u9078\u6319\u306b\u7acb\u5019\u88dc\u3057\u305f\u3068\u3044\u3046\u3053\u3068\u3067\u3059\u3002\n\u3053\u308c\u304b\u3089\u3001\u51fa\u99ac\u306b\u95a2\u3057\u3066\u306e\u8a18\u4e8b\u306b\u4e00\u822c\u304b\u3089\u5bc4\u305b\u3089\u308c\u305f\u30b3\u30e1\u30f3\u30c8\u306e\u4f8b\u3092\u6319\u3052\u307e\u3059\u3002\n\u3088\u308a\u7c21\u6f54\u3067\u8aad\u307f\u3084\u3059\u3044\u3082\u306e\u306b\u3059\u308b\u306e\u3092\u624b\u4f1d\u3063\u3066\u307b\u3057\u3044\u3002\n\u672c\u5f53\u306b\u5fc5\u8981\u306a\u5834\u5408\u306f\u30012\u3064\u306e\u5225\u3005\u306e\u8b70\u8ad6\u306b\u5206\u3051\u308b\u3053\u3068\u3082\u3067\u304d\u308b\u304c\u30011\u3064\u306e\u8b70\u8ad6\u3092\u8fd4\u3059\u306e\u304c\u6700\u5584\u3067\u3042\u308b\u3053\u3068\u304c\u591a\u3044\u3060\u308d\u3046\u3002\n\n\u7d50\u679c\u306f\u3001\u304d\u3061\u3093\u3068\u30d5\u30a9\u30fc\u30de\u30c3\u30c8\u3055\u308c\u305f\u6587\u5b57\u5217\u5f62\u5f0f\uff08strings\uff09\u306eJSON\u30ea\u30b9\u30c8\u3068\u3057\u3066\u8fd4\u3057\u3066\u304f\u3060\u3055\u3044\u3002\n\n\n/human\n\nAI\u6280\u8853\u306f\u3001\u305d\u306e\u30e9\u30a4\u30d5\u30b5\u30a4\u30af\u30eb\u306b\u304a\u3051\u308b\u74b0\u5883\u8ca0\u8377\u306e\u4f4e\u6e1b\u306b\u91cd\u70b9\u3092\u7f6e\u3044\u3066\u958b\u767a\u3055\u308c\u308b\u3079\u304d\u3067\u3042\u308b\u3002\n\n/ai \n\n[\n  \"\u79c1\u305f\u3061\u306f\u3001AI\u6280\u8853\u304c\u74b0\u5883\u306b\u4e0e\u3048\u308b\u5f71\u97ff\u306e\u8efd\u6e1b\u306b\u7126\u70b9\u3092\u5f53\u3066\u308b\u3079\u304d\u3067\u3042\u308b\"\n]\n\n/human \n\nAI\u306e\u80fd\u529b\u3001\u9650\u754c\u3001\u502b\u7406\u7684\u914d\u616e\u306b\u3064\u3044\u3066\u4e00\u822c\u306e\u4eba\u3005\u3092\u6559\u80b2\u3059\u308b\u305f\u3081\u306e\u5354\u8abf\u7684\u306a\u52aa\u529b\u304c\u5fc5\u8981\u3067\u3042\u308b\u3002\n\n/ai \n\n[\n  \"AI\u306e\u80fd\u529b\u306b\u3064\u3044\u3066\u4e00\u822c\u306e\u4eba\u3005\u3092\u6559\u80b2\u3059\u3079\u304d\u3067\u3042\u308b\u3002\",\n  \"AI\u306e\u9650\u754c\u3068\u502b\u7406\u7684\u914d\u616e\u306b\u3064\u3044\u3066\u3001\u4e00\u822c\u306e\u4eba\u3005\u3092\u6559\u80b2\u3059\u3079\u304d\u3067\u3042\u308b\u3002\"\n]\n\n/human \n\nAI\u306f\u3001\u30a8\u30cd\u30eb\u30ae\u30fc\u52b9\u7387\u3068\u5c45\u4f4f\u8005\u306e\u30a6\u30a7\u30eb\u30d3\u30fc\u30a4\u30f3\u30b0\u306e\u305f\u3081\u306b\u30b9\u30de\u30fc\u30c8\u30db\u30fc\u30e0\u3084\u30d3\u30eb\u3092\u6700\u9069\u5316\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u308b\u3002\n\n/ ai \n\n[\n  \"AI\u306f\u3001\u30a8\u30cd\u30eb\u30ae\u30fc\u52b9\u7387\u3068\u5c45\u4f4f\u8005\u306e\u30a6\u30a7\u30eb\u30d3\u30fc\u30a4\u30f3\u30b0\u306e\u305f\u3081\u306b\u30b9\u30de\u30fc\u30c8\u30db\u30fc\u30e0\u3084\u30d3\u30eb\u3092\u6700\u9069\u5316\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u308b\u3002\"\n]\n\n/human \n\nAI\u306f\u30a8\u30cd\u30eb\u30ae\u30fc\u30fb\u30b0\u30ea\u30c3\u30c9\u3092\u6700\u9069\u5316\u3057\u3001\u7121\u99c4\u3068\u4e8c\u9178\u5316\u70ad\u7d20\u6392\u51fa\u3092\u524a\u6e1b\u3059\u308b\u306e\u306b\u5f79\u7acb\u3064\u3002\n\n/ai \n\n[\n  \"AI\u306f\u30a8\u30cd\u30eb\u30ae\u30fc\u7db2\u3092\u6700\u9069\u5316\u3057\u3001\u7121\u99c4\u3068\u4e8c\u9178\u5316\u70ad\u7d20\u6392\u51fa\u3092\u524a\u6e1b\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u308b\u3002\"\n]\n",
        "model": "gpt-4o"
      }
    },
    {
      "step": "embedding",
      "completed": "2024-09-04T20:50:13.941959",
      "duration": 5.463115,
      "params": {
        "source_code": "\nfrom langchain.embeddings import OpenAIEmbeddings\nimport pandas as pd\nfrom tqdm import tqdm\n\n\ndef embedding(config):\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/embeddings.pkl\"\n    arguments = pd.read_csv(f\"outputs/{dataset}/args.csv\")\n    embeddings = []\n    for i in tqdm(range(0, len(arguments), 1000)):\n        args = arguments[\"argument\"].tolist()[i: i + 1000]\n        embeds = OpenAIEmbeddings().embed_documents(args)\n        embeddings.extend(embeds)\n    df = pd.DataFrame(\n        [\n            {\"arg-id\": arguments.iloc[i][\"arg-id\"], \"embedding\": e}\n            for i, e in enumerate(embeddings)\n        ]\n    )\n    df.to_pickle(path)\n"
      }
    },
    {
      "step": "clustering",
      "completed": "2024-09-04T20:50:29.062489",
      "duration": 15.120105,
      "params": {
        "clusters": 8,
        "source_code": "\"\"\"Cluster the arguments using UMAP + HDBSCAN and GPT-4.\"\"\"\n\nimport pandas as pd\nimport numpy as np\nfrom importlib import import_module\n\n\ndef clustering(config):\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/clusters.csv\"\n    arguments_df = pd.read_csv(f\"outputs/{dataset}/args.csv\")\n    arguments_array = arguments_df[\"argument\"].values\n\n    embeddings_df = pd.read_pickle(f\"outputs/{dataset}/embeddings.pkl\")\n    embeddings_array = np.asarray(embeddings_df[\"embedding\"].values.tolist())\n    clusters = config['clustering']['clusters']\n\n    result = cluster_embeddings(\n        docs=arguments_array,\n        embeddings=embeddings_array,\n        metadatas={\n            \"arg-id\": arguments_df[\"arg-id\"].values,\n            \"comment-id\": arguments_df[\"comment-id\"].values,\n        },\n        n_topics=clusters,\n    )\n    result.to_csv(path, index=False)\n\n\ndef cluster_embeddings(\n    docs,\n    embeddings,\n    metadatas,\n    min_cluster_size=2,\n    n_components=2,\n    n_topics=6,\n):\n    # (!) we import the following modules dynamically for a reason\n    # (they are slow to load and not required for all pipelines)\n    SpectralClustering = import_module('sklearn.cluster').SpectralClustering\n    stopwords = import_module('nltk.corpus').stopwords\n    HDBSCAN = import_module('hdbscan').HDBSCAN\n    UMAP = import_module('umap').UMAP\n    CountVectorizer = import_module(\n        'sklearn.feature_extraction.text').CountVectorizer\n    BERTopic = import_module('bertopic').BERTopic\n\n    umap_model = UMAP(\n        random_state=42,\n        n_components=n_components,\n    )\n    hdbscan_model = HDBSCAN(min_cluster_size=min_cluster_size)\n\n    stop = stopwords.words(\"english\")\n    vectorizer_model = CountVectorizer(stop_words=stop)\n    topic_model = BERTopic(\n        umap_model=umap_model,\n        hdbscan_model=hdbscan_model,\n        vectorizer_model=vectorizer_model,\n        verbose=True,\n    )\n\n    # Fit the topic model.\n    _, __ = topic_model.fit_transform(docs, embeddings=embeddings)\n\n    n_samples = len(embeddings)\n    n_neighbors = min(n_samples - 1, 10)\n    spectral_model = SpectralClustering(\n        n_clusters=n_topics,\n        affinity=\"nearest_neighbors\",\n        n_neighbors=n_neighbors,  # Use the modified n_neighbors\n        random_state=42\n    )\n    umap_embeds = umap_model.fit_transform(embeddings)\n    cluster_labels = spectral_model.fit_predict(umap_embeds)\n\n    result = topic_model.get_document_info(\n        docs=docs,\n        metadata={\n            **metadatas,\n            \"x\": umap_embeds[:, 0],\n            \"y\": umap_embeds[:, 1],\n        },\n    )\n\n    result.columns = [c.lower() for c in result.columns]\n    result = result[['arg-id', 'x', 'y', 'probability']]\n    result['cluster-id'] = cluster_labels\n\n    return result\n"
      }
    },
    {
      "step": "labelling",
      "completed": "2024-09-04T20:50:46.757563",
      "duration": 17.694241,
      "params": {
        "sample_size": 30,
        "source_code": "\"\"\"Create labels for the clusters.\"\"\"\n\nfrom tqdm import tqdm\nfrom typing import List\nimport numpy as np\nimport pandas as pd\nfrom langchain.chat_models import ChatOpenAI\nfrom utils import messages, update_progress\n\n\ndef labelling(config):\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/labels.csv\"\n\n    arguments = pd.read_csv(f\"outputs/{dataset}/args.csv\")\n    clusters = pd.read_csv(f\"outputs/{dataset}/clusters.csv\")\n\n    results = pd.DataFrame()\n\n    sample_size = config['labelling']['sample_size']\n    prompt = config['labelling']['prompt']\n    model = config['labelling']['model']\n\n    question = config['question']\n    cluster_ids = clusters['cluster-id'].unique()\n\n    update_progress(config, total=len(cluster_ids))\n\n    for _, cluster_id in tqdm(enumerate(cluster_ids), total=len(cluster_ids)):\n        args_ids = clusters[clusters['cluster-id']\n                            == cluster_id]['arg-id'].values\n        args_ids = np.random.choice(args_ids, size=min(\n            len(args_ids), sample_size), replace=False)\n        args_sample = arguments[arguments['arg-id']\n                                .isin(args_ids)]['argument'].values\n\n        args_ids_outside = clusters[clusters['cluster-id']\n                                    != cluster_id]['arg-id'].values\n        args_ids_outside = np.random.choice(args_ids_outside, size=min(\n            len(args_ids_outside), sample_size), replace=False)\n        args_sample_outside = arguments[arguments['arg-id']\n                                        .isin(args_ids_outside)]['argument'].values\n\n        label = generate_label(question, args_sample,\n                               args_sample_outside, prompt, model)\n        results = pd.concat([results, pd.DataFrame(\n            [{'cluster-id': cluster_id, 'label': label}])], ignore_index=True)\n        update_progress(config, incr=1)\n\n    results.to_csv(path, index=False)\n\n\ndef generate_label(question, args_sample, args_sample_outside, prompt, model):\n    llm = ChatOpenAI(model_name=model, temperature=0.0)\n    outside = '\\n * ' + '\\n * '.join(args_sample_outside)\n    inside = '\\n * ' + '\\n * '.join(args_sample)\n    input = f\"Question of the consultation:{question}\\n\\n\" + \\\n        f\"Examples of arguments OUTSIDE the cluster:\\n {outside}\" + \\\n        f\"Examples of arguments INSIDE the cluster:\\n {inside}\"\n    response = llm(messages=messages(prompt, input)).content.strip()\n    return response\n",
        "prompt": "/system \n\n\u3042\u306a\u305f\u306f\u3001\u3088\u308a\u5e83\u7bc4\u306a\u30b3\u30f3\u30b5\u30eb\u30c6\u30fc\u30b7\u30e7\u30f3\u5185\u306e\u4e00\u9023\u306e\u8b70\u8ad6\u306b\u5bfe\u3059\u308b\u30ab\u30c6\u30b4\u30ea\u30e9\u30d9\u30eb\u3092\u751f\u6210\u3059\u308b\u30ab\u30c6\u30b4\u30ea\u30e9\u30d9\u30ea\u30f3\u30b0\u30a2\u30b7\u30b9\u30bf\u30f3\u30c8\u3067\u3059\u3002\u3042\u306a\u305f\u306b\u306f\u3001\u76f8\u8ac7\u306e\u4e3b\u306a\u8cea\u554f\u3001\u30af\u30e9\u30b9\u30bf\u5185\u306e\u8b70\u8ad6\u306e\u30ea\u30b9\u30c8\u3001\u304a\u3088\u3073\u3053\u306e\u30af\u30e9\u30b9\u30bf\u5916\u306e\u8b70\u8ad6\u306e\u30ea\u30b9\u30c8\u304c\u4e0e\u3048\u3089\u308c\u307e\u3059\u3002\u3042\u306a\u305f\u306f\u30af\u30e9\u30b9\u30bf\u30fc\u3092\u8981\u7d04\u3059\u308b1\u3064\u306e\u30ab\u30c6\u30b4\u30ea\u30fc\u30e9\u30d9\u30eb\u3067\u56de\u7b54\u3057\u307e\u3059\u3002\n\n\u3042\u306a\u305f\u306f\u3001\u3088\u308a\u5e83\u7bc4\u306a\u30b3\u30f3\u30b5\u30eb\u30c6\u30fc\u30b7\u30e7\u30f3\u5185\u306e\u4e00\u9023\u306e\u8b70\u8ad6\u306b\u5bfe\u3059\u308b\u30ab\u30c6\u30b4\u30ea\u30e9\u30d9\u30eb\u3092\u751f\u6210\u3059\u308b\u30ab\u30c6\u30b4\u30ea\u30e9\u30d9\u30ea\u30f3\u30b0\u30a2\u30b7\u30b9\u30bf\u30f3\u30c8\u3067\u3059\u3002\u3042\u306a\u305f\u306b\u306f\u3001\u76f8\u8ac7\u306e\u4e3b\u306a\u8cea\u554f\u3001\u30af\u30e9\u30b9\u30bf\u5185\u306e\u8b70\u8ad6\u306e\u30ea\u30b9\u30c8\u3001\u304a\u3088\u3073\u3053\u306e\u30af\u30e9\u30b9\u30bf\u5916\u306e\u8b70\u8ad6\u306e\u30ea\u30b9\u30c8\u304c\u4e0e\u3048\u3089\u308c\u307e\u3059\u3002\u3042\u306a\u305f\u306f\u30af\u30e9\u30b9\u30bf\u30fc\u3092\u8981\u7d04\u3059\u308b1\u3064\u306e\u30ab\u30c6\u30b4\u30ea\u30fc\u30e9\u30d9\u30eb\u3067\u56de\u7b54\u3057\u307e\u3059\u3002\n\n\u30e9\u30d9\u30eb\u306f\u975e\u5e38\u306b\u7c21\u6f54\u3067\u306a\u3051\u308c\u3070\u306a\u3089\u305a\u3001\u30af\u30e9\u30b9\u30bf\u30fc\u3068\u305d\u306e\u5916\u5074\u306b\u3042\u308b\u8ad6\u70b9\u3092\u533a\u5225\u3059\u308b\u306e\u306b\u5341\u5206\u306a\u6b63\u78ba\u3055\u3067\u306a\u3051\u308c\u3070\u306a\u3089\u306a\u3044\u3002\n\n/human\n\n\u30b3\u30f3\u30b5\u30eb\u30c6\u30fc\u30b7\u30e7\u30f3\u306e\u8cea\u554f \u300c\u82f1\u56fd\u306eEU\u96e2\u8131\u6c7a\u5b9a\u306e\u5f71\u97ff\u306f\u4f55\u3060\u3068\u601d\u3044\u307e\u3059\u304b\uff1f\n\n\u95a2\u5fc3\u306e\u3042\u308b\u30af\u30e9\u30b9\u30bf\u30fc\u4ee5\u5916\u306e\u8ad6\u70b9\u306e\u4f8b\n\n * \u30a8\u30e9\u30b9\u30e0\u30b9\u30fb\u30d7\u30ed\u30b0\u30e9\u30e0\u304b\u3089\u306e\u9664\u5916\u306b\u3088\u308a\u3001\u6559\u80b2\u30fb\u6587\u5316\u4ea4\u6d41\u306e\u6a5f\u4f1a\u304c\u5236\u9650\u3055\u308c\u305f\u3002\n * \u82f1\u56fd\u306f\u3001\u56fd\u5883\u691c\u554f\u306e\u5f37\u5316\u306b\u3088\u308b\u65c5\u884c\u6642\u9593\u306e\u5ef6\u9577\u306b\u5bfe\u51e6\u3057\u3001\u901a\u52e4\u5ba2\u3084\u65c5\u884c\u5ba2\u306b\u5f71\u97ff\u3092\u4e0e\u3048\u305f\u3002\n * \u74b0\u5883\u57fa\u6e96\u306b\u304a\u3051\u308b\u5354\u529b\u304c\u6e1b\u5c11\u3057\u3001\u6c17\u5019\u5909\u52d5\u3068\u95d8\u3046\u52aa\u529b\u304c\u59a8\u3052\u3089\u308c\u305f\u3002\n * \u76f8\u4e92\u533b\u7642\u5354\u5b9a\u306e\u4e2d\u65ad\u306b\u3088\u308a\u3001\u60a3\u8005\u30b1\u30a2\u306b\u8ab2\u984c\u3092\u611f\u3058\u305f\u3002\n * Brexit\u95a2\u9023\u306e\u5909\u66f4\u306b\u3088\u308a\u3001\u5bb6\u65cf\u306e\u5c45\u4f4f\u6a29\u3084\u5e02\u6c11\u6a29\u306e\u7533\u8acb\u304c\u8907\u96d1\u306b\u306a\u3063\u305f\u3002\n * \u82f1\u56fd\u306f\u3001\u5171\u540c\u7814\u7a76\u6a5f\u4f1a\u306e\u6e1b\u5c11\u306b\u3088\u308a\u3001\u7814\u7a76\u306e\u8ab2\u984c\u306b\u53d6\u308a\u7d44\u3080\u4e16\u754c\u7684\u306a\u53d6\u308a\u7d44\u307f\u306b\u652f\u969c\u3092\u304d\u305f\u3059\u3053\u3068\u3092\u76ee\u306e\u5f53\u305f\u308a\u306b\u3057\u305f\u3002\n * EU\u306e\u6587\u5316\u52a9\u6210\u30d7\u30ed\u30b0\u30e9\u30e0\u304b\u3089\u306e\u9664\u5916\u306b\u3088\u308a\u3001\u5275\u9020\u7684\u306a\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u306e\u5236\u9650\u306b\u76f4\u9762\u3057\u305f\u3002\n * \u82f1\u56fd\u306f\u3001EU\u306e\u8cc7\u91d1\u63d0\u4f9b\u306e\u55aa\u5931\u306b\u3088\u308a\u3001\u6148\u5584\u6d3b\u52d5\u3084\u30b3\u30df\u30e5\u30cb\u30c6\u30a3\u652f\u63f4\u306e\u5f8c\u9000\u3092\u76ee\u306e\u5f53\u305f\u308a\u306b\u3057\u305f\u3002\n * \u6d88\u8cbb\u8005\u4fdd\u8b77\u306e\u5f31\u4f53\u5316\u306b\u3088\u308a\u3001\u56fd\u5883\u3092\u8d8a\u3048\u305f\u7d1b\u4e89\u89e3\u6c7a\u306b\u8ab2\u984c\u304c\u751f\u3058\u305f\u3002\n * \u82f1\u56fd\u306f\u30d7\u30ed\u306e\u97f3\u697d\u5bb6\u3068\u3057\u3066EU\u8af8\u56fd\u3092\u30c4\u30a2\u30fc\u3059\u308b\u969b\u306e\u5236\u9650\u306b\u76f4\u9762\u3057\u3001\u30ad\u30e3\u30ea\u30a2\u306b\u5f71\u97ff\u3092\u4e0e\u3048\u305f\u3002\n\n\u30af\u30e9\u30b9\u30bf\u30fc\u5185\u90e8\u3067\u306e\u8b70\u8ad6\u306e\u4f8b\n\n * Brexit\u306b\u3088\u308a\u30b5\u30d7\u30e9\u30a4\u30c1\u30a7\u30fc\u30f3\u304c\u6df7\u4e71\u3057\u3001\u4f01\u696d\u306b\u3068\u3063\u3066\u30b3\u30b9\u30c8\u5897\u3068\u7d0d\u671f\u9045\u5ef6\u306b\u3064\u306a\u304c\u3063\u305f\u3002\n * \u30d6\u30ec\u30b0\u30b8\u30c3\u30c8\u306e\u305f\u3081\u3001\u5e02\u5834\u306e\u5909\u52d5\u3084\u6295\u8cc7\u30fb\u9000\u8077\u91d1\u306e\u4e0d\u78ba\u5b9f\u6027\u306b\u76f4\u9762\u3057\u305f\u3002\n * \u65b0\u305f\u306a\u95a2\u7a0e\u3084\u901a\u95a2\u624b\u7d9a\u304d\u306b\u3088\u308a\u3001\u82f1\u56fd\u306f\u8f38\u51fa\u696d\u8005\u3068\u3057\u3066\u5229\u76ca\u7387\u306e\u4f4e\u4e0b\u306b\u5bfe\u51e6\u3057\u305f\u3002\n * \u30d6\u30ec\u30b0\u30b8\u30c3\u30c8\u5f8c\u3001\u4f01\u696d\u304cEU\u5e02\u5834\u5185\u306b\u3068\u3069\u307e\u308b\u305f\u3081\u306b\u4e8b\u696d\u3092\u79fb\u8ee2\u3057\u305f\u305f\u3081\u3001\u96c7\u7528\u3092\u5931\u3063\u305f\u3002\n * \u82f1\u56fd\u306f\u8f38\u5165\u54c1\u4fa1\u683c\u306e\u9ad8\u9a30\u306b\u3088\u308b\u751f\u6d3b\u8cbb\u306e\u5897\u52a0\u306b\u82e6\u3057\u3093\u3060\u3002\n * \u82f1\u56fd\u306e\u30cf\u30a4\u30c6\u30af\u7523\u696d\u3078\u306e\u6295\u8cc7\u304c\u6e1b\u5c11\u3057\u3001\u6280\u8853\u9769\u65b0\u3068\u96c7\u7528\u6a5f\u4f1a\u306b\u5f71\u97ff\u3092\u4e0e\u3048\u305f\u3002\n * \u65b0\u305f\u306a\u30d3\u30b6\u898f\u5236\u306b\u3088\u308b\u89b3\u5149\u5ba2\u306e\u6e1b\u5c11\u3092\u76ee\u306e\u5f53\u305f\u308a\u306b\u3057\u3001\u63a5\u5ba2\u696d\u306b\u5f71\u97ff\u3002\n * \u30dd\u30f3\u30c9\u4fa1\u5024\u306e\u4e0b\u843d\u306b\u3088\u308a\u8cfc\u8cb7\u529b\u304c\u4f4e\u4e0b\u3057\u3001\u65c5\u8cbb\u304c\u5897\u52a0\u3057\u305f\u3002\n\n\n/ai \n\n\u8ca1\u52d9\u4e0a\u306e\u30de\u30a4\u30ca\u30b9\u5f71\u97ff",
        "model": "gpt-4o"
      }
    },
    {
      "step": "takeaways",
      "completed": "2024-09-04T20:51:09.820495",
      "duration": 23.061965,
      "params": {
        "sample_size": 30,
        "source_code": "\"\"\"Create summaries for the clusters.\"\"\"\n\nfrom tqdm import tqdm\nimport os\nfrom typing import List\nimport numpy as np\nimport pandas as pd\nfrom langchain.chat_models import ChatOpenAI\nfrom utils import messages, update_progress\n\n\ndef takeaways(config):\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/takeaways.csv\"\n\n    arguments = pd.read_csv(f\"outputs/{dataset}/args.csv\")\n    clusters = pd.read_csv(f\"outputs/{dataset}/clusters.csv\")\n\n    results = pd.DataFrame()\n\n    sample_size = config['takeaways']['sample_size']\n    prompt = config['takeaways']['prompt']\n    model = config['takeaways']['model']\n\n    model = config.get('model_takeaways', config.get('model', 'gpt3.5-turbo'))\n    cluster_ids = clusters['cluster-id'].unique()\n\n    update_progress(config, total=len(cluster_ids))\n\n    for _, cluster_id in tqdm(enumerate(cluster_ids), total=len(cluster_ids)):\n        args_ids = clusters[clusters['cluster-id']\n                            == cluster_id]['arg-id'].values\n        args_ids = np.random.choice(args_ids, size=min(\n            len(args_ids), sample_size), replace=False)\n        args_sample = arguments[arguments['arg-id']\n                                .isin(args_ids)]['argument'].values\n        label = generate_takeaways(args_sample, prompt, model)\n        results = pd.concat([results, pd.DataFrame(\n            [{'cluster-id': cluster_id, 'takeaways': label}])], ignore_index=True)\n        update_progress(config, incr=1)\n\n    results.to_csv(path, index=False)\n\n\ndef generate_takeaways(args_sample, prompt, model):\n    llm = ChatOpenAI(model_name=model, temperature=0.0)\n    input = \"\\n\".join(args_sample)\n    response = llm(messages=messages(prompt, input)).content.strip()\n    return response\n",
        "prompt": "/system \n\n\u3042\u306a\u305f\u306f\u30b7\u30f3\u30af\u30bf\u30f3\u30af\u3067\u50cd\u304f\u5c02\u9580\u5bb6\u30ea\u30b5\u30fc\u30c1\u30a2\u30b7\u30b9\u30bf\u30f3\u30c8\u3067\u3059\u3002\u3042\u306a\u305f\u306f\u3001\u516c\u958b\u5354\u8b70\u306e\u969b\u306b\u53c2\u52a0\u8005\u306e\u4e00\u56e3\u304b\u3089\u51fa\u3055\u308c\u305f\u8b70\u8ad6\u306e\u30ea\u30b9\u30c8\u3092\u6e21\u3055\u308c\u307e\u3059\u3002\u3042\u306a\u305f\u306f\u3001\u4e3b\u306a\u8ad6\u70b9\u30921\uff5e2\u6bb5\u843d\u306b\u307e\u3068\u3081\u3066\u56de\u7b54\u3057\u307e\u3059\u3002\u3042\u306a\u305f\u306f\u3068\u3066\u3082\u7c21\u6f54\u3067\u3001\u8aad\u307f\u3084\u3059\u3044\u77ed\u3044\u6587\u7ae0\u3092\u66f8\u304f\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002\n \n/human\n\n[\n  \"\u9283\u306b\u3088\u308b\u66b4\u529b\u306f\u3001\u79c1\u305f\u3061\u306e\u793e\u4f1a\u306b\u304a\u3051\u308b\u6df1\u523b\u306a\u516c\u8846\u885b\u751f\u306e\u5371\u6a5f\u3092\u69cb\u6210\u3057\u3066\u3044\u308b\u3068\u56fa\u304f\u4fe1\u3058\u3066\u3044\u307e\u3059\u3002\",\n  \"\u5305\u62ec\u7684\u306a\u9283\u898f\u5236\u7b56\u3092\u901a\u3058\u3066\u3001\u3053\u306e\u554f\u984c\u306b\u65e9\u6025\u306b\u53d6\u308a\u7d44\u3080\u5fc5\u8981\u304c\u3042\u308b\u3002\",\n  \"\u3059\u3079\u3066\u306e\u9283\u8cfc\u5165\u8005\u306b\u5bfe\u3059\u308b\u8eab\u5143\u8abf\u67fb\u306e\u5b9f\u65bd\u3092\u652f\u6301\u3057\u307e\u3059\u3002\",\n  \"\u30a2\u30b5\u30eb\u30c8\u30fb\u30a6\u30a7\u30dd\u30f3\u3068\u5927\u5bb9\u91cf\u5f3e\u5009\u306e\u7981\u6b62\u306b\u8cdb\u6210\u3057\u307e\u3059\u3002\",\n  \"\u9055\u6cd5\u306a\u9283\u306e\u58f2\u8cb7\u3092\u9632\u3050\u305f\u3081\u3001\u3088\u308a\u53b3\u3057\u3044\u898f\u5236\u3092\u63d0\u5531\u3057\u307e\u3059\u3002\",\n  \"\u9283\u306e\u8cfc\u5165\u30d7\u30ed\u30bb\u30b9\u306b\u304a\u3044\u3066\u3001\u7cbe\u795e\u9451\u5b9a\u3092\u7fa9\u52d9\u4ed8\u3051\u308b\u3079\u304d\u3067\u3042\u308b\u3002\"\n]\n\n/ai \n\n\u53c2\u52a0\u8005\u306f\u3001\u5305\u62ec\u7684\u306a\u9283\u898f\u5236\u3092\u6c42\u3081\u3001\u666e\u904d\u7684\u306a\u8eab\u5143\u8abf\u67fb\u3001\u7a81\u6483\u5175\u5668\u306e\u7981\u6b62\u3001\u9055\u6cd5\u306a\u9283\u58f2\u8cb7\u306e\u6291\u5236\u3001\u7cbe\u795e\u885b\u751f\u8a55\u4fa1\u306e\u512a\u5148\u306a\u3069\u3092\u5f37\u8abf\u3057\u305f\u3002",
        "model": "gpt-4o"
      }
    },
    {
      "step": "overview",
      "completed": "2024-09-04T20:51:12.786233",
      "duration": 2.96482,
      "params": {
        "source_code": "\"\"\"Create summaries for the clusters.\"\"\"\n\nfrom tqdm import tqdm\nimport os\nfrom typing import List\nimport numpy as np\nimport pandas as pd\nfrom langchain.chat_models import ChatOpenAI\nfrom utils import messages, update_progress\n\n\ndef overview(config):\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/overview.txt\"\n\n    takeaways = pd.read_csv(f\"outputs/{dataset}/takeaways.csv\")\n    labels = pd.read_csv(f\"outputs/{dataset}/labels.csv\")\n\n    prompt = config['overview']['prompt']\n    model = config['overview']['model']\n\n    ids = labels['cluster-id'].to_list()\n    takeaways.set_index('cluster-id', inplace=True)\n    labels.set_index('cluster-id', inplace=True)\n\n    input = ''\n    for i, id in enumerate(ids):\n        input += f\"# Cluster {i}/{len(ids)}: {labels.loc[id]['label']}\\n\\n\"\n        input += takeaways.loc[id]['takeaways'] + '\\n\\n'\n\n    llm = ChatOpenAI(model_name=model, temperature=0.0)\n    response = llm(messages=messages(prompt, input)).content.strip()\n\n    with open(path, 'w') as file:\n        file.write(response)\n",
        "prompt": "/system \n\n\u3042\u306a\u305f\u306f\u30b7\u30f3\u30af\u30bf\u30f3\u30af\u3067\u50cd\u304f\u5c02\u9580\u5bb6\u30ea\u30b5\u30fc\u30c1\u30a2\u30b7\u30b9\u30bf\u30f3\u30c8\u3067\u3059\u3002\n\u3042\u306a\u305f\u306e\u30c1\u30fc\u30e0\u306f\u3001\u3042\u308b\u30c8\u30d4\u30c3\u30af\u306b\u95a2\u3059\u308b\u516c\u958b\u30b3\u30f3\u30b5\u30eb\u30c6\u30fc\u30b7\u30e7\u30f3\u3092\u5b9f\u65bd\u3057\u3001\u3055\u307e\u3056\u307e\u306a\u9078\u629e\u80a2\u306e\u30af\u30e9\u30b9\u30bf\u30fc\u3092\u5206\u6790\u3057\u59cb\u3081\u307e\u3057\u305f\u3002\n\u3042\u306a\u305f\u306f\u4eca\u3001\u30af\u30e9\u30b9\u30bf\u30fc\u306e\u30ea\u30b9\u30c8\u3068\u5404\u30af\u30e9\u30b9\u30bf\u30fc\u306e\u7c21\u5358\u306a\u5206\u6790\u3092\u53d7\u3051\u53d6\u308a\u307e\u3059\u3002\n\u3042\u306a\u305f\u306e\u4ed5\u4e8b\u306f\u3001\u305d\u306e\u7d50\u679c\u3092\u7c21\u6f54\u306b\u307e\u3068\u3081\u308b\u3053\u3068\u3067\u3059\u3002\n\u3042\u306a\u305f\u306e\u8981\u7d04\u306f\u975e\u5e38\u306b\u7c21\u6f54\u3067\u306a\u3051\u308c\u3070\u306a\u3089\u305a\uff08\u305b\u3044\u305c\u30441\u6bb5\u843d\u3001\u305b\u3044\u305c\u30444\u6587\uff09\u3001\u5e73\u51e1\u306a\u8868\u73fe\u306f\u907f\u3051\u306a\u3051\u308c\u3070\u306a\u308a\u307e\u305b\u3093\u3002",
        "model": "gpt-4o"
      }
    },
    {
      "step": "translation",
      "completed": "2024-09-04T20:51:12.789629",
      "duration": 0.00162,
      "params": {
        "model": "gpt-4o",
        "languages": [],
        "flags": [],
        "source_code": "import json\nfrom tqdm import tqdm\nimport pandas as pd\nfrom langchain.chat_models import ChatOpenAI\nfrom utils import messages, t\nfrom langchain.schema import AIMessage\nimport pandas as pd\nimport json\nfrom tqdm import tqdm\n\ndef translation(config):\n\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/translations.json\"\n    results = {}\n\n    languages = list(config.get('translation', {}).get('languages', []))\n    if len(languages) == 0:\n        print(\"No languages specified. Skipping translation step.\")\n        # creating an empty file any, to reduce special casing later\n        with open(path, 'w') as file:\n            json.dump(results, file, indent=2)\n        return\n\n    arguments = pd.read_csv(f\"outputs/{dataset}/args.csv\")\n    labels = pd.read_csv(f\"outputs/{dataset}/labels.csv\")\n    takeaways = pd.read_csv(f\"outputs/{dataset}/takeaways.csv\")\n    with open(f\"outputs/{dataset}/overview.txt\") as f:\n        overview = f.read()\n\n    UI_copy = [t(\"Argument\"), t(\"Original comment\"), t(\"Representative arguments\"),\n               t(\"Open full-screen map\"), t(\"Back to report\"), t(\"Hide labels\"), t(\"Show labels\"),\n               t(\"Show filters\"), t(\"Hide filters\"), t(\"Min. votes\"), t(\"Consensus\"),\n               t(\"Showing\"), t(\"arguments\"), t(\"Reset zoom\"), t(\"Click anywhere on the map to close this\"),\n               t(\"Click on the dot for details\"), t(\"agree\"), t(\"disagree\"), t(\"Language\"),\n               t(\"English\"), t(\"arguments\"), t(\"of total\"), t(\"Overview\"), t(\"Cluster analysis\"),\n               t(\"Representative comments\"), t(\"Introduction\"), t(\"Clusters\"), t(\"Appendix\"),\n               t(\"This report was generated using an AI pipeline that consists of the following steps\"),\n               t(\"Step\"), t(\"extraction\"), t(\"show code\"), t(\"hide code\"), t(\"show prompt\"),\n               t(\"hide prompt\"), t(\"embedding\"), t(\"clustering\"), t(\"labelling\"), t(\"takeaways\"),\n               t(\"overview\")]\n\n    arg_list = arguments['argument'].to_list() + \\\n        labels['label'].to_list() + \\\n        UI_copy + \\\n        languages\n\n    if 'name' in config:\n        arg_list.append(config['name'])\n    if 'question' in config:\n        arg_list.append(config['question'])\n\n    prompt_file = config.get('translation_prompt', 'default')\n    with open(f\"prompts/translation/{prompt_file}.txt\") as f:\n        prompt = f.read()\n    model = config['model']\n\n    config['translation_prompt'] = prompt\n\n    translations = [translate_lang(\n        arg_list, 10, prompt, lang, model) for lang in languages]\n\n    # handling long takeaways differently, WITHOUT batching too much\n    long_arg_list = takeaways['takeaways'].to_list()\n    long_arg_list.append(overview)\n    if 'intro' in config:\n        long_arg_list.append(config['intro'])\n\n    long_translations = [translate_lang(\n        long_arg_list, 1, prompt, lang, model) for lang in languages]\n\n    for i, id in enumerate(arg_list):\n        print('i, id', i, id)\n        results[str(id)] = list([t[i] for t in translations])\n    for i, id in enumerate(long_arg_list):\n        results[str(id)] = list([t[i] for t in long_translations])\n\n    with open(path, 'w') as file:\n        json.dump(results, file, indent=2)\n\n\ndef translate_lang(arg_list, batch_size, prompt, lang, model):\n    translations = []\n    lang_prompt = prompt.replace(\"{language}\", lang)\n    print(f\"Translating to {lang}...\")\n    for i in tqdm(range(0, len(arg_list), batch_size)):\n        batch = arg_list[i: i + batch_size]\n        translations.extend(translate_batch(batch, lang_prompt, model))\n    return translations\n\n\ndef translate_batch(batch, lang_prompt, model, retries=3):\n    llm = ChatOpenAI(model_name=model, temperature=0.0)\n    input = json.dumps(list(batch))\n    response = llm(messages=messages(lang_prompt, input)).content.strip()\n    if \"```\" in response:\n        response = response.split(\"```\")[1]\n    if response.startswith(\"json\"):\n        response = response[4:]\n    try:\n        parsed = [a.strip() for a in json.loads(response)]\n        if len(parsed) != len(batch):\n            print(\"Warning: batch size mismatch!\")\n            print(\"Batch len:\", len(batch))\n            print(\"Response len:\", len(parsed))\n            for i, item in enumerate(batch):\n                print(f\"Batch item {i}:\", item)\n                if (i < len(parsed)):\n                    print(\"Response:\", parsed[i])\n            if (len(batch) > 1):\n                print(\"Retrying with smaller batches...\")\n                mid = len(batch) // 2\n                return translate_batch(batch[:mid], lang_prompt, model, retries - 1) + \\\n                    translate_batch(\n                        batch[mid:], lang_prompt, model, retries - 1)\n            else:\n                print(\"Retrying batch...\")\n                return translate_batch(batch, lang_prompt, model, retries - 1)\n        else:\n            return parsed\n    except json.decoder.JSONDecodeError as e:\n        print(\"JSON error:\", e)\n        print(\"Response was:\", response)\n        if retries > 0:\n            print(\"Retrying batch...\")\n            return translate_batch(batch, lang_prompt, model, retries - 1)\n        else:\n            raise e",
        "prompt": "/system \n\n\u3042\u306a\u305f\u306f\u30d7\u30ed\u306e\u7ffb\u8a33\u8005\u3067\u3059\u3002\n\u65e5\u672c\u8a9e\u3067\u66f8\u304b\u308c\u305f\u5358\u8a9e\u3068\u6587\u7ae0\u306e\u30ea\u30b9\u30c8\u3092\u53d7\u3051\u53d6\u308a\u307e\u3059\u3002\n\u540c\u3058\u30ea\u30b9\u30c8\u3092\u540c\u3058\u9806\u756a\u3067\u3001{language}\u306b\u7ffb\u8a33\u3057\u3066\u8fd4\u3057\u3066\u304f\u3060\u3055\u3044\u3002\n\u5143\u306e\u30ea\u30b9\u30c8\u3068\u540c\u3058\u9577\u3055\u306e\u6587\u5b57\u5217\u306e\u6709\u52b9\u306aJSON\u30ea\u30b9\u30c8\u3092\u8fd4\u3059\u3088\u3046\u306b\u3057\u3066\u304f\u3060\u3055\u3044\u3002"
      }
    },
    {
      "step": "aggregation",
      "completed": "2024-09-04T20:51:13.629938",
      "duration": 0.839114,
      "params": {
        "source_code": "\"\"\"Generate a convenient JSON output file.\"\"\"\n\nfrom tqdm import tqdm\nfrom typing import List\nimport pandas as pd\nfrom langchain.chat_models import ChatOpenAI\nimport json\n\n\ndef aggregation(config):\n\n    path = f\"outputs/{config['output_dir']}/result.json\"\n\n    results = {\n        \"clusters\": [],\n        \"comments\": {},\n        \"translations\": {},\n        \"overview\": \"\",\n        \"config\": config,\n    }\n\n    arguments = pd.read_csv(f\"outputs/{config['output_dir']}/args.csv\")\n    arguments.set_index('arg-id', inplace=True)\n\n    comments = pd.read_csv(f\"inputs/{config['input']}.csv\")\n    useful_comment_ids = set(arguments['comment-id'].values)\n    for _, row in comments.iterrows():\n        id = row['comment-id']\n        if id in useful_comment_ids:\n            res = {'comment': row['comment-body']}\n            numeric_cols = ['agrees', 'disagrees']\n            string_cols = ['video', 'interview', 'timestamp']\n            for col in numeric_cols:\n                if col in row:\n                    res[col] = float(row[col])\n            for col in string_cols:\n                if col in row:\n                    res[col] = row[col]\n            results['comments'][str(id)] = res\n\n    languages = list(config.get('translation', {}).get('languages', []))\n    if len(languages) > 0:\n        with open(f\"outputs/{config['output_dir']}/translations.json\") as f:\n            translations = f.read()\n        results['translations'] = json.loads(translations)\n\n    clusters = pd.read_csv(f\"outputs/{config['output_dir']}/clusters.csv\")\n    labels = pd.read_csv(f\"outputs/{config['output_dir']}/labels.csv\")\n    takeaways = pd.read_csv(f\"outputs/{config['output_dir']}/takeaways.csv\")\n    takeaways.set_index('cluster-id', inplace=True)\n\n    with open(f\"outputs/{config['output_dir']}/overview.txt\") as f:\n        overview = f.read()\n    results['overview'] = overview\n\n    for _, row in labels.iterrows():\n        cid = row['cluster-id']\n        label = row['label']\n        arg_rows = clusters[clusters['cluster-id'] == cid]\n        arguments_in_cluster = []\n        for _, arg_row in arg_rows.iterrows():\n            arg_id = arg_row['arg-id']\n            argument = arguments.loc[arg_id]['argument']\n            comment_id = arguments.loc[arg_id]['comment-id']\n            x = float(arg_row['x'])\n            y = float(arg_row['y'])\n            p = float(arg_row['probability'])\n            obj = {\n                'arg_id': arg_id,\n                'argument': argument,\n                'comment_id': str(comment_id),\n                'x': x,\n                'y': y,\n                'p': p,\n            }\n            arguments_in_cluster.append(obj)\n        results['clusters'].append({\n            'cluster': label,\n            'cluster_id': str(cid),\n            'takeaways': takeaways.loc[cid]['takeaways'],\n            'arguments': arguments_in_cluster\n        })\n\n    with open(path, 'w') as file:\n        json.dump(results, file, indent=2)\n"
      }
    },
    {
      "step": "visualization",
      "completed": "2024-09-04T20:51:19.524972",
      "duration": 5.894551,
      "params": {
        "replacements": [],
        "source_code": "\nimport subprocess\n\n\ndef visualization(config):\n    output_dir = config['output_dir']\n    with open(f\"outputs/{output_dir}/result.json\") as f:\n        result = f.read()\n\n    cwd = \"../next-app\"\n    command = f\"REPORT={output_dir} npm run build\"\n\n    try:\n        process = subprocess.Popen(command, shell=True, cwd=cwd, stdout=subprocess.PIPE,\n                                   stderr=subprocess.PIPE, universal_newlines=True)\n        while True:\n            output_line = process.stdout.readline()\n            if output_line == '' and process.poll() is not None:\n                break\n            if output_line:\n                print(output_line.strip())\n        process.wait()\n        errors = process.stderr.read()\n        if errors:\n            print(\"Errors:\")\n            print(errors)\n    except subprocess.CalledProcessError as e:\n        print(\"Error: \", e)\n"
      }
    }
  ],
  "lock_until": "2024-09-04T20:56:19.526664",
  "current_job": "visualization",
  "current_job_started": "2024-09-04T20:51:13.630491",
  "end_time": "2024-09-04T20:51:19.526655"
}